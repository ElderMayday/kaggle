{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 Statistical foundations of machine learning. Kaggle project report. Aldar Saranov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "The following project was implemented in order to implement an apartment price prediction for the Kaggle competition. It was implemented in R language and divided in several project parts - feature selection, model selection, model combination, model evaluation and result production. The report contains the main snippets of the project. The rest can be found at the [GitHub repository][1].\n",
    "\n",
    "\n",
    "[1]: https://github.com/ElderMayday/kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "Many features (attributes) are supposed to contribute the prediction value and still some of them should be considered inconsistent or inefficient. In order to select the desired features one can apply either **filter methods** (i.e. regarding the features independently) or **wrapper methods** (i.e. regarding subsets of feature). Still for the sake of simplicity we use filter methods. As a grade of feature \"usefullness\" we use information grade measure (IG). This measure is based on the notion of entropy. Entropy itself characterizes the grade of system's disorder.\n",
    "\n",
    "$$H = - \\sum_{i=1}^{n} p_i log_b (p_i), \\textit{where } p_i \\textit{ - probability of i-th outcome}$$\n",
    "\n",
    "IG while performing a transition from state 1 to state 2 is as follows.\n",
    "\n",
    "$$IG(1\\rightarrow2) = H(1) - H(2)$$\n",
    "\n",
    "After computing this value for every feature separatedly we can order the features and slice the most important ones.\n",
    "**Warning!** To run the code one must download the whole repository with data files and correct the folder paths by setwd(), plus install the used libraries for the jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'FSelector' was built under R version 3.3.3\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_name feature_gain\n",
      "6   OverallQual    0.4922011\n",
      "5  Neighborhood    0.4569727\n",
      "22    GrLivArea    0.3825791\n",
      "31   GarageCars    0.3281534\n",
      "32   GarageArea    0.3210374\n",
      "7     YearBuilt    0.2922323\n",
      "15     BsmtQual    0.2870105\n",
      "13    ExterQual    0.2837421\n",
      "24  KitchenQual    0.2770460\n",
      "18  TotalBsmtSF    0.2765359\n",
      "29  GarageYrBlt    0.2569508\n",
      "1    MSSubClass    0.2566161\n",
      "30 GarageFinish    0.2501103\n",
      "20    X1stFlrSF    0.2394652\n",
      "23     FullBath    0.2339366\n",
      "28   GarageType    0.2144176\n",
      "8  YearRemodAdd    0.2112479\n",
      "27  FireplaceQu    0.1963499\n",
      "14   Foundation    0.1933651\n",
      "25 TotRmsAbvGrd    0.1731205\n",
      "21    X2ndFlrSF    0.1728490\n",
      "3   LotFrontage    0.1547592\n",
      "26   Fireplaces    0.1531060\n",
      "16 BsmtFinType1    0.1519456\n",
      "4       LotArea    0.1443197\n",
      "33  OpenPorchSF    0.1438957\n",
      "9   Exterior1st    0.1430904\n",
      "10  Exterior2nd    0.1408458\n",
      "19    HeatingQC    0.1329305\n",
      "17   BsmtFinSF1    0.1155452\n",
      "2      MSZoning    0.1111115\n",
      "12   MasVnrArea    0.1031449\n",
      "11   MasVnrType    0.1027884\n"
     ]
    }
   ],
   "source": [
    "library(FSelector)   #load the feature-selection library\n",
    "\n",
    "setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!\n",
    "\n",
    "#takes dataframe and shows the features with satisfactory information gain\n",
    "feature_selector <- function(data)\n",
    "{\n",
    "  #calculate the information gain of each feature\n",
    "  features <- information.gain(SalePrice~., data)\n",
    "  \n",
    "  #result dataframe\n",
    "  result = data.frame()\n",
    "  \n",
    "  row_names = row.names(features)\n",
    "  \n",
    "  #select every feature with IF higher than 0.1\n",
    "  for (i in 1:nrow(features))\n",
    "  {\n",
    "    if (features[i, 1] > 0.1)\n",
    "    {\n",
    "      result <- rbind(result, data.frame(\"feature_name\"= row_names[i], \"feature_gain\" = features[i, 1]))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  #order the features\n",
    "  result = result[rev(order(result$feature_gain)),]\n",
    "\n",
    "  #print the selected features\n",
    "  print(result)\n",
    "  \n",
    "  return(result)\n",
    "}\n",
    "\n",
    "#load the train data\n",
    "train = read.csv(\"./train.csv\", header = TRUE)\n",
    "\n",
    "#do feature selection\n",
    "features = feature_selector(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
