{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 Statistical foundations of machine learning. Kaggle project report. Aldar Saranov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "The following project was implemented in order to implement an apartment price prediction for the Kaggle competition. It was divided in several project parts - feature selection, model selection, model combination, model evaluation. The report will describe the implementation and description of each of these steps and analyze project details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "The project was implemented in R language (version 3.3.4). This report contains the main snippets of the project. The rest can be found at the [GitHub repository][1]. As a feature selection information gain filtering was used. As model selection cross-validation was applied. And finally models are combined by means of weighted average over the described models. In addition to these strategies we apply factored feature reassignment and conduct some analyzis concerning the impact of the configuration hyperparameters at the model estimate. The results have been computed and uploaded with total score 0.14474 on the test set and 0.1533264 on the train set. The submission can be found at Kaggle by nickname \"INFOF422_Saranov\" or \"Aldar Saranov\" at the leaderboard.\n",
    "\n",
    "[1]: https://github.com/ElderMayday/kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Many features (attributes) are supposed to contribute the prediction value and still some of them should be considered inconsistent or inefficient. In order to select the desired features one can apply either **filter methods** (i.e. regarding the features independently) or **wrapper methods** (i.e. regarding subsets of feature). Still for the sake of simplicity we use filter methods. As a grade of feature \"usefullness\" we use information grade measure (IG). This measure is based on the notion of entropy. Entropy itself characterizes the grade of system's disorder.\n",
    "\n",
    "$$H = - \\sum_{i=1}^{n} p_i log_b (p_i), \\textit{where } p_i \\textit{ - probability of i-th outcome}$$\n",
    "\n",
    "IG while performing a transition from state 1 to state 2 is as follows.\n",
    "\n",
    "$$IG(1\\rightarrow2) = H(1) - H(2)$$\n",
    "\n",
    "After computing this value for every feature separatedly we can order the features and slice the most important ones.\n",
    "**Warning!** To run the code one must download the whole repository with data files and correct the folder paths by setwd(), plus install the used libraries for the jupyter.\n",
    "\n",
    "The following code performs the feature selection procedure. It uses FSelector package which allows to compute IG vector for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'FSelector' was built under R version 3.3.3\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_name feature_gain\n",
      "6   OverallQual    0.4922011\n",
      "5  Neighborhood    0.4569727\n",
      "22    GrLivArea    0.3825791\n",
      "31   GarageCars    0.3281534\n",
      "32   GarageArea    0.3210374\n",
      "7     YearBuilt    0.2922323\n",
      "15     BsmtQual    0.2870105\n",
      "13    ExterQual    0.2837421\n",
      "24  KitchenQual    0.2770460\n",
      "18  TotalBsmtSF    0.2765359\n",
      "29  GarageYrBlt    0.2569508\n",
      "1    MSSubClass    0.2566161\n",
      "30 GarageFinish    0.2501103\n",
      "20    X1stFlrSF    0.2394652\n",
      "23     FullBath    0.2339366\n",
      "28   GarageType    0.2144176\n",
      "8  YearRemodAdd    0.2112479\n",
      "27  FireplaceQu    0.1963499\n",
      "14   Foundation    0.1933651\n",
      "25 TotRmsAbvGrd    0.1731205\n",
      "21    X2ndFlrSF    0.1728490\n",
      "3   LotFrontage    0.1547592\n",
      "26   Fireplaces    0.1531060\n",
      "16 BsmtFinType1    0.1519456\n",
      "4       LotArea    0.1443197\n",
      "33  OpenPorchSF    0.1438957\n",
      "9   Exterior1st    0.1430904\n",
      "10  Exterior2nd    0.1408458\n",
      "19    HeatingQC    0.1329305\n",
      "17   BsmtFinSF1    0.1155452\n",
      "2      MSZoning    0.1111115\n",
      "12   MasVnrArea    0.1031449\n",
      "11   MasVnrType    0.1027884\n"
     ]
    }
   ],
   "source": [
    "library(FSelector)   #load the feature-selection library\n",
    "\n",
    "setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!\n",
    "\n",
    "#takes dataframe and shows the features with satisfactory information gain\n",
    "feature_selector <- function(data)\n",
    "{\n",
    "  #calculate the information gain of each feature\n",
    "  features <- information.gain(SalePrice~., data)\n",
    "  \n",
    "  #result dataframe\n",
    "  result = data.frame()\n",
    "  \n",
    "  row_names = row.names(features)\n",
    "  \n",
    "  #select every feature with IF higher than 0.1\n",
    "  for (i in 1:nrow(features))\n",
    "  {\n",
    "    if (features[i, 1] > 0.1)\n",
    "    {\n",
    "      result <- rbind(result, data.frame(\"feature_name\"= row_names[i], \"feature_gain\" = features[i, 1]))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  #order the features\n",
    "  result = result[rev(order(result$feature_gain)),]\n",
    "\n",
    "  #print the selected features\n",
    "  print(result)\n",
    "  \n",
    "  return(result)\n",
    "}\n",
    "\n",
    "#load the train data\n",
    "train = read.csv(\"./train.csv\", header = TRUE)\n",
    "\n",
    "#do feature selection\n",
    "features = feature_selector(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have a table of the features ordered by IG importance that have IG higher than 0.1 and therefore passed the feature selection process. The rest features are ignored since they are considered to cause randomness in the regression problem solving.\n",
    "\n",
    "**Factor values reassignment**\n",
    "\n",
    "In order to improve prediction based on factor features we can reassign the factor features (such as \"FireplaceQu\") to a scalar integer values where higher values generally correspond to higher prices and the lower ones to the lower prices. Unlike the initial values these scalar values will more correlate to the sale price and hence reduce the variance of the prediction. To achieve that we use agregating over the train set by the corresponding factor features. The following snippet represents factored feature aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Group.1         x\n",
      "11 MeadowV  98576.47\n",
      "10  IDOTRR 100123.78\n",
      "3   BrDale 104493.75\n",
      "4  BrkSide 124834.05\n",
      "8  Edwards 128219.70\n",
      "18 OldTown 128225.30\n",
      "19  Sawyer 136793.14\n",
      "2  Blueste 137500.00\n",
      "23   SWISU 142591.36\n",
      "15 NPkVill 142694.44\n",
      "13   NAmes 145847.08\n",
      "12 Mitchel 156270.12\n",
      "20 SawyerW 186555.80\n",
      "17  NWAmes 189050.07\n",
      "9  Gilbert 192854.51\n",
      "1  Blmngtn 194870.88\n",
      "6  CollgCr 197965.77\n",
      "7  Crawfor 210624.73\n",
      "5  ClearCr 212565.43\n",
      "21 Somerst 225379.84\n",
      "25 Veenker 238772.73\n",
      "24  Timber 242247.45\n",
      "22 StoneBr 310499.00\n",
      "16 NridgHt 316270.62\n",
      "14 NoRidge 335295.32\n"
     ]
    }
   ],
   "source": [
    "train = read.csv(\"./train.csv\", header = TRUE)\n",
    "\n",
    "group = aggregate(train[,c('SalePrice')], list(train[,'Neighborhood']), mean)  #group feature values by average price\n",
    "group = group[order(group$x),]\n",
    "\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above represents the 'Neighborhood' feature reassignment. 'MeadowV' thus is substituted to 1 and 'NoRidge' to 25. By applying similar aggregation to every factored feature, afterwards we substitute every of these factors to its ordered number which thus will correlate and contribute the price prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "With each another run the result may differ a little bit since the cross-validation procedure contains a randomizing step. Lets consider following problem notation:\n",
    "\n",
    "$$D=((x_1,y_1), (x_2,y_2), ... , (x_n,y_n)) \\textit{ - the train set to be taught.}$$\n",
    "\n",
    "To estimate the model precision we permutate the observations in random mannet and split them into m folds k observations each.\n",
    "\n",
    "$$F_1 = ((x_{1,1}, y_{1,1}),...,(x_{1,k}, y_{1,k})), ..., F_m = ((x_{m,1}, y_{m,1}),...,(x_{m,k}, y_{m,k}))$$\n",
    "\n",
    "Then for iterations i from 1 to m we apply a training-evaluating step.\n",
    "\n",
    "$$F_i \\textit{ - current test set, } F_i^t = \\bigcup \\limits_{\\forall j \\neq i} {F_j} \\textit{ - current train set.}$$\n",
    "\n",
    "$$\\textit{Let } E_i \\textit{ be the estimate of i-th iteration}$$\n",
    "\n",
    "The total estimate can be assessed as the mean of these estimates:\n",
    "\n",
    "$$E = \\frac {1} {m} \\sum \\limits_{i=1}^{m} {E_i} $$\n",
    "\n",
    "As an estimate we picked rooted MSE of logarithmic values since it is the one which is used in Kaggle system.\n",
    "\n",
    "**Implementation**\n",
    "The cross-validation procedure was done manually without applying any side libraries. It includes split-folds function which permutates the data set and packs it into several folds which have size difference equal to one at most. In model-selection procedure the cross-validation is applied to every of the candidate model configurations and the model which had the best (lowest) estimate is chosen for the final usage for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree_index = \"\n",
      "[1] 9\n",
      "[1] \"tree_rmse = \"\n",
      "[1] 0.5835374\n",
      "[1] \"lazy_index = \"\n",
      "[1] 6\n",
      "[1] \"lazy_rmse = \"\n",
      "[1] 0.2210736\n",
      "[1] \"svm_index = \"\n",
      "[1] 33\n",
      "[1] \"svm_rmse = \"\n",
      "[1] 0.9607639\n"
     ]
    }
   ],
   "source": [
    "library(lazy)\n",
    "library(tree)\n",
    "library(e1071)\n",
    "\n",
    "\n",
    "\n",
    "setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!\n",
    "\n",
    "source(\"parameters.R\")\n",
    "source(\"split-folds.R\")\n",
    "source(\"feature-filter.R\")\n",
    "source(\"teach-model.R\")\n",
    "\n",
    "\n",
    "\n",
    "#teaches and evaluates an abstract model using cross-validation technique over the given [folds] with current [param] configuration\n",
    "cross_validation <- function(folds, model_flag, param)\n",
    "{\n",
    "  mse_all = c()\n",
    "  \n",
    "  iteration = 1\n",
    "  \n",
    "  while (iteration <= length(folds))\n",
    "  {\n",
    "    test = folds[[iteration]]\n",
    "    \n",
    "    train = data.frame()\n",
    "    \n",
    "    fold_num = 1\n",
    "    \n",
    "    while (fold_num <= length(folds))\n",
    "    {\n",
    "      if (fold_num != iteration)\n",
    "        train <- rbind(train, folds[[fold_num]])\n",
    "      \n",
    "      fold_num = fold_num + 1\n",
    "    }\n",
    "    \n",
    "    model = teach_model(train, model_flag, param)\n",
    "    \n",
    "    prediction = test[,1:(ncol(test)-1)]\n",
    "    predicted = predict(model, prediction)\n",
    "    mse = mean((log2(predicted[[1]]) - log2(test[,\"SalePrice\"]))^2)\n",
    "    mse_all = c(mse_all, mse)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "  }\n",
    "  \n",
    "  return(sqrt(mse))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#apply model selection to [train] dataframe with [model_flag] model over [param] configuration set\n",
    "select_model <- function(train, model_flag, param)\n",
    "{\n",
    "  folds = split_folds(train)\n",
    "  \n",
    "  rmse_all = c()\n",
    "  \n",
    "  for (i in 1:nrow(param))\n",
    "  {\n",
    "    #print(i)          #uncomment if you want to track selection progress\n",
    "    rmse = cross_validation(folds, model_flag, param[i, ])\n",
    "    rmse_all = c(rmse_all, rmse)\n",
    "  }\n",
    "  \n",
    "  min_index = which.min(rmse_all)\n",
    "  min_value = min(rmse_all)\n",
    "  \n",
    "  param[,'rmse'] = rmse_all\n",
    "  \n",
    "  return(param)\n",
    "}\n",
    "\n",
    "\n",
    "train_unfiltered = read.csv(\"./train.csv\", header = TRUE)\n",
    "train = feature_filter(train_unfiltered)\n",
    "train = reassign_factors(train, train)\n",
    "train = replace_na(train)\n",
    "\n",
    "parameters_tree = get_parameters_tree()\n",
    "result_tree = select_model(train, 1, parameters_tree)\n",
    "print('tree_index = ')\n",
    "print(which.min(result_tree[,'rmse']))\n",
    "print('tree_rmse = ')\n",
    "print(min(result_tree[,'rmse']))\n",
    "\n",
    "parameters_lazy = get_parameters_lazy()\n",
    "result_lazy = select_model(train, 2, parameters_lazy)\n",
    "print('lazy_index = ')\n",
    "print(which.min(result_lazy[,'rmse']))\n",
    "print('lazy_rmse = ')\n",
    "print(min(result_lazy[,'rmse']))\n",
    "\n",
    "parameters_svm = get_parameters_svm()\n",
    "result_svm = select_model(train, 3, parameters_svm)\n",
    "print('svm_index = ')\n",
    "print(which.min(result_svm[,'rmse']))\n",
    "print('svm_rmse = ')\n",
    "print(min(result_svm[,'rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model configuration analyzis**\n",
    "We create predefined sets of models configurations for each of the models that are ranged within subjectively reasonable intervals. Later on we analyze the parameter impact on the prediction to justify the choice.\n",
    "\n",
    "*Regression trees*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.023509\n",
      "[1] 0.1533264\n"
     ]
    }
   ],
   "source": [
    "#performs model combination, outputs the result on test data, estimates mse on train data\n",
    "\n",
    "library(lazy)\n",
    "library(tree)\n",
    "library(e1071)\n",
    "\n",
    "setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!\n",
    "\n",
    "source(\"parameters.R\")\n",
    "source(\"feature-filter.R\")\n",
    "source(\"teach-model.R\")\n",
    "\n",
    "combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)\n",
    "{\n",
    "  p1 = 1 / rmse_tree\n",
    "  p2 = 1/ rmse_lazy\n",
    "  p3 = 1/ rmse_svm\n",
    "  \n",
    "  s = p1 + p2 + p3\n",
    "  w1 = p1 / s\n",
    "  w2 = p2 / s\n",
    "  w3 = p3 / s\n",
    "  \n",
    "  predicted_tree = predict(model_tree, data)\n",
    "  predicted_lazy = predict(model_lazy, data)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element\n",
    "  predicted_svm = predict(model_svm, data)\n",
    "  \n",
    "  predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3\n",
    "  \n",
    "  return(predicted)\n",
    "}\n",
    "\n",
    "train_raw = read.csv(\"./train.csv\", header = TRUE)\n",
    "train = feature_filter(train_raw)\n",
    "train = reassign_factors(train, train)\n",
    "train = replace_na(train)\n",
    "\n",
    "\n",
    "#assign hardcoded precomputated indexes of the best model configurations\n",
    "tree_conf_id = 9\n",
    "lazy_conf_id = 6\n",
    "svm_conf_id = 33\n",
    "\n",
    "tree_parameters = get_parameters_tree()[tree_conf_id,]\n",
    "lazy_parameters = get_parameters_lazy()[lazy_conf_id,]\n",
    "svm_parameters = get_parameters_svm()[svm_conf_id,]\n",
    "\n",
    "rmse_tree = 0.5835374\n",
    "rmse_lazy = 0.2210736\n",
    "rmse_svm = 0.9607639\n",
    "\n",
    "model_tree = teach_model(train, 1, tree_parameters)\n",
    "model_lazy = teach_model(train, 2, lazy_parameters)\n",
    "model_svm = teach_model(train, 3, svm_parameters)\n",
    "\n",
    "#apply to train set (to evaluate combined rmse)\n",
    "\n",
    "data = replace_na(train)[,1:(ncol(train)-1)]   #train data without SalePrice column\n",
    "predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)\n",
    "\n",
    "mse = mean((log2(predicted) - log2(train[,'SalePrice']))^2)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
