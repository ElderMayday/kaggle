model_svm = teach_model(train, 3, svm_parameters)
predicted = combine(train, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
test_raw = read.csv("./test.csv", header = TRUE)
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
test = feature_filter_without_saleprice(test_raw)
test = reassign_factors(train, test)
test = replace_na(test)
predicted = combine(test, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
View(test)
predicted = combine(test, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("feature-filter.R")
source("teach-model.R")
combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
{
p1 = 1 / rmse_tree
p2 = 1/ rmse_lazy
p3 = 1/ rmse_svm
s = p1 + p2 + p3
w1 = p1 / s
w2 = p2 / s
w3 = p3 / s
prediction = data[,1:(ncol(data)-1)]
predicted_tree = predict(model_tree, prediction)
prediction = data[,1:(ncol(data)-1)]
predicted_lazy = predict(model_lazy, prediction)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element
prediction = data[,1:(ncol(data)-1)]
predicted_svm = predict(model_svm, prediction)
predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3
return(predicted)
}
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
train = reassign_factors(train, train)
train = replace_na(train)
tree_conf_id = 54
lazy_conf_id = 18
svm_conf_id = 1
tree_parameters = get_parameters_tree()[tree_conf_id,]
lazy_parameters = get_parameters_lazy()[lazy_conf_id,]
svm_parameters = get_parameters_svm()[svm_conf_id,]
rmse_tree = 71361
rmse_lazy = 30307
rmse_svm = 116588
model_tree = teach_model(train, 1, tree_parameters)
model_lazy = teach_model(train, 2, lazy_parameters)
model_svm = teach_model(train, 3, svm_parameters)
predicted = combine(train, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
train = replace_na(train)[,1:(ncol(data)-1)]
train = replace_na(train)[,1:(ncol(train)-1)]
View(train)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("feature-filter.R")
source("teach-model.R")
combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
{
p1 = 1 / rmse_tree
p2 = 1/ rmse_lazy
p3 = 1/ rmse_svm
s = p1 + p2 + p3
w1 = p1 / s
w2 = p2 / s
w3 = p3 / s
predicted_tree = predict(model_tree, data)
predicted_lazy = predict(model_lazy, data)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element
predicted_svm = predict(model_svm, data)
predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3
return(predicted)
}
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
train = reassign_factors(train, train)
train = replace_na(train)
tree_conf_id = 54
lazy_conf_id = 18
svm_conf_id = 1
tree_parameters = get_parameters_tree()[tree_conf_id,]
lazy_parameters = get_parameters_lazy()[lazy_conf_id,]
svm_parameters = get_parameters_svm()[svm_conf_id,]
rmse_tree = 71361
rmse_lazy = 30307
rmse_svm = 116588
model_tree = teach_model(train, 1, tree_parameters)
model_lazy = teach_model(train, 2, lazy_parameters)
model_svm = teach_model(train, 3, svm_parameters)
data = replace_na(train)[,1:(ncol(train)-1)]   #train data without SalePrice column
predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
mse = mean((predicted - train[,'SalePrice'])^2)
rmse = sqrt(mse)
test_raw = read.csv("./test.csv", header = TRUE)
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
test = feature_filter_without_saleprice(test_raw)
test = reassign_factors(train, test)
data = replace_na(test)
predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
write.table(predicted, row.names = c(1461:2919), file = "out.csv", sep = ",", col.names = 'Id,SalePrice', qmethod = "double", quote = FALSE)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("split-folds.R")
source("feature-filter.R")
source("teach-model.R")
cross_validation <- function(folds, model_flag, param)
{
mse_all = c()
iteration = 1
while (iteration <= length(folds))
{
test = folds[[iteration]]
train = data.frame()
fold_num = 1
while (fold_num <= length(folds))
{
if (fold_num != iteration)
train <- rbind(train, folds[[fold_num]])
fold_num = fold_num + 1
}
model = teach_model(train, model_flag, param)
prediction = test[,1:(ncol(test)-1)]
predicted = predict(model, prediction)
mse = mean((predicted[[1]] - test[,"SalePrice"])^2)
mse_all = c(mse_all, mse)
iteration = iteration + 1
}
return(sqrt(mse))
}
select_model <- function(train, model_flag, param)
{
folds = split_folds(train)
rmse_all = c()
for (i in 1:nrow(param))
{
#print(i)          #uncomment if you want to track selection progress
rmse = cross_validation(folds, model_flag, param[i, ])
rmse_all = c(rmse_all, rmse)
}
min_index = which.min(rmse_all)
min_value = min(rmse_all)
param[,'rmse'] = rmse_all
return(param)
}
train_unfiltered = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_unfiltered)
train = reassign_factors(train, train)
train = replace_na(train)
parameters_tree = get_parameters_tree()
result_tree = select_model(train, 1, parameters_tree)
print('tree_index = ')
print(which.min(result_tree[,'rmse']))
print('tree_rmse = ')
print(min(result_tree[,'rmse']))
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("split-folds.R")
source("feature-filter.R")
source("teach-model.R")
cross_validation <- function(folds, model_flag, param)
{
mse_all = c()
iteration = 1
while (iteration <= length(folds))
{
test = folds[[iteration]]
train = data.frame()
fold_num = 1
while (fold_num <= length(folds))
{
if (fold_num != iteration)
train <- rbind(train, folds[[fold_num]])
fold_num = fold_num + 1
}
model = teach_model(train, model_flag, param)
prediction = test[,1:(ncol(test)-1)]
predicted = predict(model, prediction)
mse = mean((predicted[[1]] - test[,"SalePrice"])^2)
mse_all = c(mse_all, mse)
iteration = iteration + 1
}
return(sqrt(mse))
}
select_model <- function(train, model_flag, param)
{
folds = split_folds(train)
rmse_all = c()
for (i in 1:nrow(param))
{
#print(i)          #uncomment if you want to track selection progress
rmse = cross_validation(folds, model_flag, param[i, ])
rmse_all = c(rmse_all, rmse)
}
min_index = which.min(rmse_all)
min_value = min(rmse_all)
param[,'rmse'] = rmse_all
return(param)
}
train_unfiltered = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_unfiltered)
train = reassign_factors(train, train)
train = replace_na(train)
parameters_tree = get_parameters_tree()
result_tree = select_model(train, 1, parameters_tree)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("feature-filter.R")
source("teach-model.R")
combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
{
p1 = 1 / rmse_tree
p2 = 1/ rmse_lazy
p3 = 1/ rmse_svm
s = p1 + p2 + p3
w1 = p1 / s
w2 = p2 / s
w3 = p3 / s
predicted_tree = predict(model_tree, data)
predicted_lazy = predict(model_lazy, data)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element
predicted_svm = predict(model_svm, data)
predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3
return(predicted)
}
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
train = reassign_factors(train, train)
train = replace_na(train)
tree_conf_id = 649
lazy_conf_id = 21
svm_conf_id = 65
tree_parameters = get_parameters_tree()[tree_conf_id,]
lazy_parameters = get_parameters_lazy()[lazy_conf_id,]
svm_parameters = get_parameters_svm()[svm_conf_id,]
rmse_tree = 71361
rmse_lazy = 30307
rmse_svm = 116588
rmse_tree = 88181
rmse_lazy = 28812
rmse_svm = 86310
model_tree = teach_model(train, 1, tree_parameters)
model_lazy = teach_model(train, 2, lazy_parameters)
model_svm = teach_model(train, 3, svm_parameters)
data = replace_na(train)[,1:(ncol(train)-1)]   #train data without SalePrice column
predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
mse = mean((predicted - train[,'SalePrice'])^2)
rmse = sqrt(mse)
log2(8)
log2(8.5)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("split-folds.R")
source("feature-filter.R")
source("teach-model.R")
cross_validation <- function(folds, model_flag, param)
{
mse_all = c()
iteration = 1
while (iteration <= length(folds))
{
test = folds[[iteration]]
train = data.frame()
fold_num = 1
while (fold_num <= length(folds))
{
if (fold_num != iteration)
train <- rbind(train, folds[[fold_num]])
fold_num = fold_num + 1
}
model = teach_model(train, model_flag, param)
prediction = test[,1:(ncol(test)-1)]
predicted = predict(model, prediction)
mse = mean((log2(predicted[[1]]) - log2(test[,"SalePrice"]))^2)
mse_all = c(mse_all, mse)
iteration = iteration + 1
}
return(sqrt(mse))
}
select_model <- function(train, model_flag, param)
{
folds = split_folds(train)
rmse_all = c()
for (i in 1:nrow(param))
{
#print(i)          #uncomment if you want to track selection progress
rmse = cross_validation(folds, model_flag, param[i, ])
rmse_all = c(rmse_all, rmse)
}
min_index = which.min(rmse_all)
min_value = min(rmse_all)
param[,'rmse'] = rmse_all
return(param)
}
train_unfiltered = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_unfiltered)
train = reassign_factors(train, train)
train = replace_na(train)
parameters_svm = get_parameters_svm()
result_svm = select_model(train, 3, parameters_svm)
train_unfiltered = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_unfiltered)
train = reassign_factors(train, train)
train = replace_na(train)
parameters_tree = get_parameters_tree()
result_tree = select_model(train, 1, parameters_tree)
print('tree_index = ')
print(which.min(result_tree[,'rmse']))
print('tree_rmse = ')
print(min(result_tree[,'rmse']))
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("feature-filter.R")
source("teach-model.R")
combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
{
p1 = 1 / rmse_tree
p2 = 1/ rmse_lazy
p3 = 1/ rmse_svm
s = p1 + p2 + p3
w1 = p1 / s
w2 = p2 / s
w3 = p3 / s
predicted_tree = predict(model_tree, data)
predicted_lazy = predict(model_lazy, data)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element
predicted_svm = predict(model_svm, data)
predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3
return(predicted)
}
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
train = reassign_factors(train, train)
train = replace_na(train)
tree_conf_id = 649
lazy_conf_id = 21
svm_conf_id = 65
tree_parameters = get_parameters_tree()[tree_conf_id,]
lazy_parameters = get_parameters_lazy()[lazy_conf_id,]
svm_parameters = get_parameters_svm()[svm_conf_id,]
rmse_tree = 0.5835374
rmse_lazy = 0.2210736
rmse_svm = 0.9607639
model_tree = teach_model(train, 1, tree_parameters)
model_lazy = teach_model(train, 2, lazy_parameters)
model_svm = teach_model(train, 3, svm_parameters)
data = replace_na(train)[,1:(ncol(train)-1)]   #train data without SalePrice column
predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
mse = mean((predicted - train[,'SalePrice'])^2)
rmse = sqrt(mse)
mse = mean((log2(predicted) - log2(train[,'SalePrice']))^2)
rmse = sqrt(mse)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("feature-filter.R")
source("teach-model.R")
combine <- function(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
{
p1 = 1 / rmse_tree
p2 = 1/ rmse_lazy
p3 = 1/ rmse_svm
s = p1 + p2 + p3
w1 = p1 / s
w2 = p2 / s
w3 = p3 / s
predicted_tree = predict(model_tree, data)
predicted_lazy = predict(model_lazy, data)[[1]]  #[[1]] since lazy package implementation returns the result as a list containing a vector as the first element
predicted_svm = predict(model_svm, data)
predicted = predicted_tree * w1 + predicted_lazy * w2 + predicted_svm * w3
return(predicted)
}
train_raw = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_raw)
train = reassign_factors(train, train)
train = replace_na(train)
tree_conf_id = 9
lazy_conf_id = 6
svm_conf_id = 33
tree_parameters = get_parameters_tree()[tree_conf_id,]
lazy_parameters = get_parameters_lazy()[lazy_conf_id,]
svm_parameters = get_parameters_svm()[svm_conf_id,]
rmse_tree = 0.5835374
rmse_lazy = 0.2210736
rmse_svm = 0.9607639
model_tree = teach_model(train, 1, tree_parameters)
model_lazy = teach_model(train, 2, lazy_parameters)
model_svm = teach_model(train, 3, svm_parameters)
data = replace_na(train)[,1:(ncol(train)-1)]   #train data without SalePrice column
predicted = combine(data, model_tree, model_lazy, model_svm, rmse_tree, rmse_lazy, rmse_svm)
mse = mean((log2(predicted) - log2(train[,'SalePrice']))^2)
rmse = sqrt(mse)
parameters_tree = get_parameters_tree()
parameters_lazy = get_parameters_lazy()
parameters_svm = get_parameters_svm()
View(parameters_tree)
library(lazy)
library(tree)
library(e1071)
setwd('D:/kaggle')  #TO-MODIFY sets the defaul folder depending on the directory path!!!
source("parameters.R")
source("split-folds.R")
source("feature-filter.R")
source("teach-model.R")
#teaches and evaluates an abstract model using cross-validation technique over the given [folds] with current [param] configuration
cross_validation <- function(folds, model_flag, param)
{
mse_all = c()
iteration = 1
while (iteration <= length(folds))
{
test = folds[[iteration]]
train = data.frame()
fold_num = 1
while (fold_num <= length(folds))
{
if (fold_num != iteration)
train <- rbind(train, folds[[fold_num]])
fold_num = fold_num + 1
}
model = teach_model(train, model_flag, param)
prediction = test[,1:(ncol(test)-1)]
predicted = predict(model, prediction)
mse = mean((log2(predicted[[1]]) - log2(test[,"SalePrice"]))^2)
mse_all = c(mse_all, mse)
iteration = iteration + 1
}
return(sqrt(mse))
}
#apply model selection to [train] dataframe with [model_flag] model over [param] configuration set
select_model <- function(train, model_flag, param)
{
folds = split_folds(train)
rmse_all = c()
for (i in 1:nrow(param))
{
#print(i)          #uncomment if you want to track selection progress
rmse = cross_validation(folds, model_flag, param[i, ])
rmse_all = c(rmse_all, rmse)
}
min_index = which.min(rmse_all)
min_value = min(rmse_all)
param[,'rmse'] = rmse_all
return(param)
}
train_unfiltered = read.csv("./train.csv", header = TRUE)
train = feature_filter(train_unfiltered)
train = reassign_factors(train, train)
train = replace_na(train)
parameters_tree = get_parameters_tree()
result_tree = select_model(train, 1, parameters_tree)
print('tree_index = ')
print(which.min(result_tree[,'rmse']))
print('tree_rmse = ')
print(min(result_tree[,'rmse']))
parameters_lazy = get_parameters_lazy()
result_lazy = select_model(train, 2, parameters_lazy)
print('lazy_index = ')
print(which.min(result_lazy[,'rmse']))
print('lazy_rmse = ')
print(min(result_lazy[,'rmse']))
parameters_svm = get_parameters_svm()
result_svm = select_model(train, 3, parameters_svm)
print('svm_index = ')
print(which.min(result_svm[,'rmse']))
print('svm_rmse = ')
print(min(result_svm[,'rmse']))
parameters_svm = get_parameters_svm()
result_svm = select_model(train, 3, parameters_svm)
group = aggregate(result_tree[,c('rmse')], list(result_tree$nobs), mean) #no impact
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$mincut), mean)  # 10
plot(group[,1],group[,2])
View(parameters_tree)
group = aggregate(result_tree[,c('rmse')], list(result_tree$nobs), mean) #no impact
View(group)
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$mincut), mean)  # 10
plot(group[,1],group[,2])
View(group)
group = aggregate(result_tree[,c('rmse')], list(result_tree$minsize), mean) #~20-180
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$mindev), mean) #0.5
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$mincut), mean)  # 10
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$minsize), mean) #~20-180
plot(group[,1],group[,2])
group = aggregate(result_tree[,c('rmse')], list(result_tree$mindev), mean) #0.5
plot(group[,1],group[,2])
View(parameters_tree)
View(parameters_svm)
